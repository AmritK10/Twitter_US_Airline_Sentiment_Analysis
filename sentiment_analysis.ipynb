{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vdUBG-YMLN1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "IYbNfGZZMLN5",
    "outputId": "48ee5ca5-4e2d-446f-8945-d972c656f01e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "data=pd.read_csv(\"Tweets.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uNKn2BMwPZ9T",
    "outputId": "339a64c6-07dc-4fe3-d139-1cb1c18ddb03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 15), (3660, 15))"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(data)\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8Gbk26bMLN-"
   },
   "outputs": [],
   "source": [
    "#forming training and testing data\n",
    "xtrain=train[\"text\"]\n",
    "ytrain=train[\"airline_sentiment\"]\n",
    "xtest=test[\"text\"]\n",
    "ytest=test[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_tHyTo5gMLOC",
    "outputId": "af1889ee-e0ce-47b3-cc32-658164299bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative,neutral & positive tweets in training data:  (6910, 2311, 1759)\n",
      "number of negative,neutral & positive tweets in training data:  (2268, 788, 604)\n"
     ]
    }
   ],
   "source": [
    "#number of negative,neutral & positive tweets in training data\n",
    "neg=0\n",
    "neu=0\n",
    "pos=0\n",
    "for sent in ytrain:\n",
    "    if sent==\"negative\":\n",
    "        neg+=1\n",
    "    elif sent==\"neutral\":\n",
    "        neu+=1\n",
    "    else:\n",
    "        pos+=1\n",
    "neg,neu,pos\n",
    "print(\"number of negative,neutral & positive tweets in training data: \",(neg,neu,pos))\n",
    "neg=0\n",
    "neu=0\n",
    "pos=0\n",
    "for sent in ytest:\n",
    "    if sent==\"negative\":\n",
    "        neg+=1\n",
    "    elif sent==\"neutral\":\n",
    "        neu+=1\n",
    "    else:\n",
    "        pos+=1\n",
    "neg,neu,pos\n",
    "print(\"number of negative,neutral & positive tweets in training data: \",(neg,neu,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "5-mBxDDgRLFB",
    "outputId": "9896c0cc-b996-4766-899b-3da012a12446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading dependencies\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I_-qDT7MLOG"
   },
   "outputs": [],
   "source": [
    "#forming set of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "import string\n",
    "punctuations=list(string.punctuation)\n",
    "stop.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taWixrqWMLOI"
   },
   "outputs": [],
   "source": [
    "#creating lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDFFOuA8MLOK"
   },
   "outputs": [],
   "source": [
    "#function for converting tags into required form\n",
    "def simple_pos(tag):\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5HJcmesnMLOO",
    "outputId": "df4ff6b6-0218-4d8a-c601-eaa2ee77f824"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample of training data\n",
    "xtrain[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gOjQvmPMLOV"
   },
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "from nltk import word_tokenize\n",
    "x_train=[]\n",
    "for text in xtrain:\n",
    "    x_train.append(word_tokenize(text))\n",
    "x_test=[]\n",
    "for text in xtest:\n",
    "    x_test.append(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "YYGPuEhBMLOY",
    "outputId": "4672e546-8cd5-4562-ea67-0a4f4c7f24cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'JetBlue',\n",
       " '@',\n",
       " 'pilyoc',\n",
       " 'dont',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'my',\n",
       " 'friend',\n",
       " '@',\n",
       " 'JetBlue',\n",
       " 'like',\n",
       " 'that',\n",
       " '.',\n",
       " '#',\n",
       " 'thefutureisweird']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample after tokenization\n",
    "x_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzVZUWhyMLOb"
   },
   "outputs": [],
   "source": [
    "#function to clean data i.e. to apply lemmatization\n",
    "def clean(text_data):\n",
    "    X_Train=[]\n",
    "    for text in text_data:\n",
    "        clean_words=[]\n",
    "        for word in text:\n",
    "            if word.lower() not in stop:\n",
    "                tag=pos_tag([word])[0][1]\n",
    "                clean_word=Lemmatizer.lemmatize(word,simple_pos(tag))\n",
    "                clean_words.append(clean_word)\n",
    "        X_Train.append(clean_words)\n",
    "    return X_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5eFFaiOMLOe"
   },
   "outputs": [],
   "source": [
    "X_Train=clean(x_train)\n",
    "X_Test=clean(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "SyyEkHEMMLOg",
    "outputId": "8f7e7ae7-7c50-43f3-da4e-cecd628493b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JetBlue',\n",
       " 'pilyoc',\n",
       " 'dont',\n",
       " 'talk',\n",
       " 'friend',\n",
       " 'JetBlue',\n",
       " 'like',\n",
       " 'thefutureisweird']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample after cleaning\n",
    "X_Train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90Rr1Oa_MLOk"
   },
   "outputs": [],
   "source": [
    "#joining the words in list to form a string\n",
    "xTrain=[\" \".join(text) for text in X_Train]\n",
    "xTest=[\" \".join(text) for text in X_Test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAqFQa8PMLOn"
   },
   "outputs": [],
   "source": [
    "#applying count vectorizer to data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec=CountVectorizer(max_features=3000,ngram_range=(1,3),max_df=0.8)\n",
    "Xtrain=count_vec.fit_transform(xTrain)\n",
    "Xtest=count_vec.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "crJ38in-MLOp",
    "outputId": "d08eac6a-7532-4a3c-fd25-280e32eeb8e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bWmckyvDMLOt",
    "outputId": "a0d292f6-c1b8-4552-df5b-9bb7c8da7d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of Multinomial Naive Bayes:  0.8087431693989071\n",
      "Testing Accuracy of Multinomial Naive Bayes:  0.7718579234972678\n"
     ]
    }
   ],
   "source": [
    "#score of Multinomial Naive Bayes\n",
    "train_score=clf.score(Xtrain,ytrain)\n",
    "test_score=clf.score(Xtest,ytest)\n",
    "print(\"Training Accuracy of Multinomial Naive Bayes: \",train_score)\n",
    "print(\"Testing Accuracy of Multinomial Naive Bayes: \",test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UaQdaav-MLO7",
    "outputId": "8199c46c-638c-458a-ca9d-0a5ae94a4bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of Support Vetor Machine (SVC):  0.8794171220400728\n",
      "Testing Accuracy of Support Vetor Machine (SVC):  0.7879781420765027\n"
     ]
    }
   ],
   "source": [
    "#applying Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC(C=55,gamma=0.0025)\n",
    "svc.fit(Xtrain,ytrain)\n",
    "train_score=svc.score(Xtrain,ytrain)\n",
    "test_score=svc.score(Xtest,ytest)\n",
    "print(\"Training Accuracy of Support Vetor Machine (SVC): \",train_score)\n",
    "print(\"Testing Accuracy of Support Vetor Machine (SVC): \",test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0Z-YvBWMLO_"
   },
   "outputs": [],
   "source": [
    "#predicting using SVC classifier\n",
    "Y_train_pred=svc.predict(Xtrain)\n",
    "Y_test_pred=svc.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "4Ma4ba1fMLPC",
    "outputId": "f3215a7a-a997-4ebc-8217-719fc4ec2f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification report of SCV\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93      6910\n",
      "     neutral       0.79      0.74      0.76      2311\n",
      "    positive       0.88      0.79      0.83      1759\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     10980\n",
      "   macro avg       0.86      0.83      0.84     10980\n",
      "weighted avg       0.88      0.88      0.88     10980\n",
      "\n",
      "Training confusion matrix of SCV\n",
      "[[6545  291   74]\n",
      " [ 472 1717  122]\n",
      " [ 195  170 1394]]\n"
     ]
    }
   ],
   "source": [
    "#printing training results of SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"Training classification report of SCV\")\n",
    "print(classification_report(ytrain,Y_train_pred))\n",
    "print(\"Training confusion matrix of SCV\")\n",
    "print(confusion_matrix(ytrain,Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "91PhxGKHMLPF",
    "outputId": "aa27a1b8-1067-4120-f1fa-523b8b1726ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification report of SCV\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      2268\n",
      "     neutral       0.62      0.61      0.61       788\n",
      "    positive       0.79      0.66      0.72       604\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      3660\n",
      "   macro avg       0.75      0.72      0.73      3660\n",
      "weighted avg       0.79      0.79      0.79      3660\n",
      "\n",
      "Testing confusion matrix of SCV\n",
      "[[2007  201   60]\n",
      " [ 260  480   48]\n",
      " [ 111   96  397]]\n"
     ]
    }
   ],
   "source": [
    "#printing testing results of SVC\n",
    "print(\"Testing classification report of SCV\")\n",
    "print(classification_report(ytest,Y_test_pred))\n",
    "print(\"Testing confusion matrix of SCV\")\n",
    "print(confusion_matrix(ytest,Y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_US_airline_sentiment_analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
